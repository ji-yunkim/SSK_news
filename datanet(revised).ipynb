{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import html\n",
    "import glob\n",
    "import codecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "output_path = \"urls.csv\"\n",
    "\n",
    "f = codecs.open(output_path,\"r\",\"UTF-8\")\n",
    "url_lists_with_newline = f.readlines()\n",
    "\n",
    "urls_list = []\n",
    "urls_to_article = []\n",
    "dates = []\n",
    "reporters = []\n",
    "titles = []\n",
    "articles = []\n",
    "sitenames = []\n",
    "times=[]\n",
    "id = []\n",
    "for url_element_with_newline in url_lists_with_newline:\n",
    "    urls_list.append(url_element_with_newline.strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbering=1\n",
    "for url in urls_list:\n",
    "    url_without_http = \"\".join(url.split(\"http://\")[1:])#.split(\"/\")[0]   \n",
    "    if \"datanet.co.kr\" in url_without_http:\n",
    "        url = \"http://\" + url_without_http\n",
    "        article_url = urllib.request.urlopen(url).read()\n",
    "        soup=BeautifulSoup(article_url, \"lxml\")\n",
    "        content = Article(url, language='ko')\n",
    "        content.download()\n",
    "        content.parse()\n",
    "        \n",
    "        if content.publish_date is None:\n",
    "            date = soup.find(\"meta\", {\"property\":\"article:published_time\"})\n",
    "            date1 = soup.find(\"div\", {\"class\":\"news-info\"})\n",
    "            date2 = soup.find(\"p\", {\"class\":\"arvdate\"})\n",
    "            date3 = soup.find(\"div\",{\"class\":\"writeInfo\"})\n",
    "            \n",
    "            #print (date2)\n",
    "            \n",
    "            if date is not None:\n",
    "                dates.append(date[\"content\"].split(\"T\")[0])\n",
    "                times.append(\"\".join(date[\"content\"].split(\"T\")[1]).split(\"+\")[0])\n",
    "            elif date1 is not None:\n",
    "                dates.append(\"\".join(date1.text.split(\"입력시간\")[1]).split(\" \")[0].strip().replace(\"|\",\"\"))\n",
    "                times.append(\"\".join(date1.text.split(\"입력시간\")[1]).split(\" \")[1].strip())\n",
    "            elif date2 is not None:\n",
    "                dates.append(\"\".join(date2.text.split(\"최종편집\")[1]).strip().split(\" \")[0])\n",
    "                times.append(\"\".join(date2.text.split(\"최종편집\")[1]).strip().split(\" \")[1])\n",
    "            elif date3 is not None:\n",
    "                dates.append(\"\".join(date3.text.split(\"최종편집\")[1]).strip().split(\" \")[0].replace(\",\",\"-\"))\n",
    "                times.append(\"\".join(date3.text.split(\"최종편집\")[1]).strip().split(\" \")[1])\n",
    "            \n",
    "            #else:\n",
    "            #    dates.append(url) \n",
    "        else:\n",
    "            t = content.publish_date\n",
    "            date = t.strftime(\"%Y-%m-%d\")\n",
    "            dates.append(date)\n",
    "            \n",
    "            time = t.strftime(\"%H:%M:%S\")\n",
    "            times.append(time)\n",
    "            \n",
    "        if len(content.title) == 0:\n",
    "            title = soup.find(\"meta\", {\"property\":\"og:title\"})\n",
    "            titles.append(title[\"content\"] if title else url)\n",
    "        else:\n",
    "            titles.append(content.title)\n",
    "            \n",
    "        if len(content.authors) == 0:\n",
    "            reporter = soup.find(\"meta\", {\"property\":\"dable:author\"})\n",
    "            reporter1 = soup.find(\"div\", {\"class\":\"news-info\"})\n",
    "            reporter2 = soup.find(\"p\", {\"class\":\"arvdate\"})\n",
    "            reporter3 = soup.find(\"div\",{\"class\":\"writeInfo\"})\n",
    "            \n",
    "            if reporter is not None:\n",
    "                reporters.append(reporter[\"content\"][0:3].strip())\n",
    "            elif reporter1 is not None:\n",
    "                reporters.append(\"\".join(reporter1.text.split(\"기자\")[0])[0:4].strip())\n",
    "            elif reporter2 is not None:\n",
    "                reporters.append(\"\".join(reporter2.text.split(\"기자\")[0])[0:4].strip())\n",
    "            elif reporter3 is not None:\n",
    "                reporters.append(\"\".join(reporter3.text.split(\"기자\")[0])[0:4].strip())\n",
    "        else:\n",
    "            reporters.append(str(content.authors).replace(\"기자\",\"\").strip())\n",
    "            \n",
    "        if len(content.text) == 0:\n",
    "            article = soup.find(\"div\", {\"itemprop\":\"articleBody\"})\n",
    "            articles.append(article.text.strip().replace(\"\\xa0\", \" \").replace('\\n', \"\").replace(\"\\r\", \"\").replace(\"\\t\", \"\") if article else \"No meta article given\")\n",
    "        else:\n",
    "            articles.append(content.text.strip().replace(\"\\xa0\", \" \").replace('\\n', \"\").replace(\"\\r\", \"\").replace(\"\\t\", \"\").replace(\"\\u200b\",\"\"))\n",
    "            \n",
    "        sitenames.append(\"데이터넷\")\n",
    "        id.append(numbering)\n",
    "        numbering += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates_ = []\n",
    "for item in dates:\n",
    "    dates_.append(item.replace(\".\",\"-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['15:03:44', '09:21:57', '09:21:57', '09:21:57']\n"
     ]
    }
   ],
   "source": [
    "print (times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‘박근혜·최순실 게이트’로 촉발된 정치혼란을 이용한 사이버 공격이 급증하고 있다. 주요 정부 기관, 국회의원 홈페이지 등에 디도스 공격이 발생하는가 하면, 사회적인 이슈를 가장한 메시지로 악성코드가 유포되는 공격도 늘어나고 있다. 조기대선이 치러질 것으로 예상되는 내년 초에는 혼란한 정국을 악용한 공격이 더욱 극성을 부릴 것으로 예상된다. 한국인터넷진흥원(KISA)이 5일 발표한 ‘2017년 7대 사이버 공격 전망’에 따르면, 내년에는 산업 전반으로 맞춤형 공격이 더욱 늘어날 것으로 보이며, 특히 정치적 이슈를 이용한 공격과 정치적 목적의 사이버 테러가 증가할 것으로 예상된다. 국내에서만 사용되는 시스템의 취약점을 이용한 공격도 이어질 것이며, 신뢰도 높은 사람이나 서비스를 기반으로 한 공격이 기승을 부릴 것으로 예상된다. 지능화되고 정교해지는 공격에 대응하기 위해 KISA는 국내외 보안기업과 함께 위협 인텔리전스 네트워크를 구축하고 있으며, 정보공유와 대응공조를 유지해 공격에 효과적으로 대응할 방침을 밝혔다. 백기승 KISA 원장은 “국내 보안 기업은 물론, 글로벌 보안 기업까지 사이버 보안을 위한 정보공유와 공동대응에 협력하는 것은 매우 의미 있는 협업방어 모델”이라며 “보안업체들은 시장에서 경쟁하는 관계에 있지만, 사이버 위협에는 공동대응하면서 방어 역량을 배가하는 ‘개방, 협업, 공유’를 통해 보안 수준을 높여나가겠다”고 말했다. 백 원장은 이어 “보안위협 정보 공유에는 보안 기업 뿐 아니라 국가연구기관, ISP, 포털사, 게임업계 및 일반 기업들도 참여하고 있다”며 “정보보호는 기업의 책임만은 아니며, 사용자와 사업자 모두가 책임져야 하는 부분이다. 보안사사고에 대응하는 한편 사고를 당했을 때 빠르게 피해를 복구할 수 있는 복원력을 높일 수 있도록 준비하는 수준까지 발전해야 한다”고 강조했다. 한편 이날 발표한 ‘2017년 7대 사이버 공격 전망’은 KISA와 보안 기업이 사이버 위협정보 공유 및 침해사고 공동 대응을 위해 운영하고 있는 ‘사이버위협 인텔리전스 네트워크’를 통해 분석한 것이다. 이 커뮤니티는 KISA, 안랩, 이스트소프트, 잉카인터넷, 하우리, NSHC, 빛스캔 등 국내 보안업체가 2014년 12월부터 구성·운영하고 있으며, 지난 6월부터는 파이어아이, 포티넷, 인텔시큐리티, 마이크로소프트, 팔로알토 네트워크, 시만텍 등 해외보안업체가 참여하고 있다. 보고서는 ‘사이버위협 인텔리전스 네트워크’에 참여하는 보안업체와 공동분석을 통해 내년에 발생할 7대 사이버 공격을 선정했다. ▲산업전반으로 번지는 한국 맞춤형 공격 ▲자산관리 등 공용 소프트웨어를 통한 표적 공격 ▲한국어 지원 등 다양한 형태의 랜섬웨어 대량 유포 ▲사회기반시설 대상 사이버 테러 발생 ▲멀버타이징 공격 등 대규모 악성코드 감염기법의 지능화 ▲악성앱 등 모바일 금융 서비스에 대한 위협 증가 ▲좀비화된 사물인터넷(IoT) 기기의 무기화 등이다.\n"
     ]
    }
   ],
   "source": [
    "print (articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['“탄핵 정국 악용 사이버 공격 늘 것”', '페이스북 올해의 화제 1위는 ‘미국 대통령 선거’', '페이스북 올해의 화제 1위는 ‘미국 대통령 선거’', '페이스북 올해의 화제 1위는 ‘미국 대통령 선거’']\n"
     ]
    }
   ],
   "source": [
    "print (titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['김선애', '강석오', '강석오', '강석오']\n"
     ]
    }
   ],
   "source": [
    "print (reporters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rows = zip(id, sitenames, dates, times, reporters, titles, articles)\n",
    "\n",
    "f = codecs.open(\"datanet.tsv\",\"w\",\"UTF-8\")\n",
    "for row in rows:\n",
    "    f.write(\"\".join(str(s)+\"\\t\" for s in row)+ \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
